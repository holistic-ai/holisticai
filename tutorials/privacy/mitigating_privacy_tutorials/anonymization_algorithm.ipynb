{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using ML anonymization to defend against attribute inference attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "# Filter out all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all, we need to import the required packages to perform our privacy analysis and mitigation. You will need to have the `holisticai` package installed on your system, remember that you can install it by running: \n",
    "```bash\n",
    "!pip install holisticai[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.datasets import load_dataset\n",
    "loaded = load_dataset(dataset='adult', preprocessed=False, as_array=False)\n",
    "df = pd.DataFrame(data=loaded.data, columns=loaded.feature_names)\n",
    "df['class'] = loaded.target.apply(lambda x: 1 if x == '>50K' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Identify categorical features\n",
    "categorical_features = X.select_dtypes(include=['category']).columns\n",
    "\n",
    "# Create transformers for numerical and categorical features\n",
    "numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine transformers into a preprocessor using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, X.select_dtypes(exclude=['category']).columns),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit and transform your data using the ColumnTransformer\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train decision tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model accuracy:  0.8185075237997748\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DTC = DecisionTreeClassifier()\n",
    "DTC.fit(X_train_transformed, y_train)\n",
    "# Predict values\n",
    "y_pred = DTC.predict(X_test_transformed)\n",
    "y_proba = DTC.predict_proba(X_test_transformed)\n",
    "print('Base model accuracy: ', DTC.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlackBox Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8921076875831713\n"
     ]
    }
   ],
   "source": [
    "from holisticai.privacy.metrics import BlackBoxAttack\n",
    "\n",
    "attack_feature = 'education'\n",
    "predictions_of_attack_feature_1 = BlackBoxAttack(attack_feature, X_train, y_train, X_test, y_test)\n",
    "print(accuracy_score(X_test['education'], predictions_of_attack_feature_1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This means that for 89% of the training set, the attacked feature is inferred correctly using this attack.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anonymized data. Improving privacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.privacy.mitigation import Anonymize\n",
    "\n",
    "X_train = X_train.set_index(pd.Series(range(len(X_train))))\n",
    "features = X_train.columns\n",
    "QI = ['education', 'marital-status', 'age']\n",
    "anonymizer = Anonymize(100, QI, categorical_features=list(categorical_features), features_names=features)\n",
    "anon = anonymizer.anonymize(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train decision tree model on anonymized data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform your data using the ColumnTransformer\n",
    "X_train_transformed_anon = preprocessor.fit_transform(anon)\n",
    "X_test_transformed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymized model accuracy:  0.8026410072678882\n"
     ]
    }
   ],
   "source": [
    "DTC_anon = DecisionTreeClassifier()\n",
    "DTC_anon.fit(X_train_transformed_anon, y_train)\n",
    "# Predict values\n",
    "y_pred_anon = DTC_anon.predict(X_test_transformed)\n",
    "y_proba_anon = DTC_anon.predict_proba(X_test_transformed)\n",
    "\n",
    "print('Anonymized model accuracy: ', DTC_anon.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BlackBox Attack on Anonymized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5658716347630259\n"
     ]
    }
   ],
   "source": [
    "# Extract dtypes from DataFrame 1\n",
    "dtypes_to_apply = X_train.dtypes.to_dict()\n",
    "# Set dtypes for all columns in DataFrame 2 based on DataFrame 1\n",
    "anon = anon.astype(dtypes_to_apply)\n",
    "\n",
    "attack_feature = 'education'\n",
    "predictions_of_attack_feature_2 = BlackBoxAttack(attack_feature, anon, y_train, X_test, y_test)\n",
    "print(accuracy_score(X_test['education'], predictions_of_attack_feature_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This means that for 56% of the training set, the attacked feature is inferred correctly using this attack.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
