{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **How Much Your Model's Accuracy is Sensitive to Changes in the Dataset's Structure?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a variety of reasons, a machine learning (ML) practitioner may be exposed to dataset shift or loss of instances in his/her dataset. This set of experiments aims to show how intrinsically exposed the practitioner is to variations in accuracy, considering inherent characteristics of the dataset that are not revealed when traditional ML experiments are conducted. A deeper look will be taken into the geometric structure of some datasets, and a method to measure the practitioner's exposure to significant changes in his/her dataset will be proposed, called **accuracy degradation profile** (ADP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from holisticai.robustness.metrics import (\n",
    "    accuracy_degradation_profile,\n",
    "    accuracy_degradation_factor,\n",
    "    pre_process_data,\n",
    ")\n",
    "\n",
    "from holisticai.robustness.plots import (\n",
    "                                    plot_2d,\n",
    "                                    plot_adp_and_adf,\n",
    "                                    plot_label_and_prediction,\n",
    "                                    plot_neighborhood,\n",
    "                                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create your own 2D classification dataset for experiments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# Generate synthetic data using make_blobs with closer centers\n",
    "center_box = 2.5\n",
    "X, y = make_blobs(n_samples=100, \n",
    "                  centers=4, \n",
    "                  n_features=2, \n",
    "                  cluster_std=0.8, \n",
    "                  center_box=(-center_box, center_box), \n",
    "                  random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D classification data\n",
    "plot_2d(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform the usual machine learning pipeline to infer the test accuracy over the **entire** test set using a Decision Tree. You may change the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train, X_test, y_train, y_test, test_indices = pre_process_data(X, y, test_size = 0.3, random_state = random_state)\n",
    "\n",
    "# Train a classifier over the data\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Accuracy over the test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy test set: {accuracy_test:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us highlight the test set on the graph. It will be important to manage the possible changes on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the test set on the plot\n",
    "plot_2d(X, y, highlight_group=test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us maintain just the test set on the graph with a (very!) small index per point for tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test set with index\n",
    "plot_2d(X, y, highlight_group=test_indices, show_just_group=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now plot y_test and y_pred together in the same graph. The values of y_pred (shaded circles) are shifted vertically by a small amount to allow better visualization. It is possible to see where the classifier uncorrectedly classified the true labels **by the different collors** between y_test and y_pred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot y_test and y_pred simultaneously\n",
    "plot_label_and_prediction(X_test, y_test, y_pred, vertical_offset=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now analyze the neighborhood of a list of selected points in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose the size of the neighborhood\n",
    "# n_neighbors = 4\n",
    "\n",
    "# # Choose the points of interest\n",
    "# # Example: [4, 76, 89]\n",
    "# points_of_interest = [96, 76, 89]\n",
    "\n",
    "# # Plot points of interest and its neighborhoods\n",
    "# plot_neighborhood(X_test,\n",
    "#                   y_test,\n",
    "#                   y_pred,\n",
    "#                   n_neighbors,\n",
    "#                   points_of_interest = points_of_interest,\n",
    "#                   vertical_offset = 0.1,\n",
    "#                   indices_show = test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the accuracies calculated in each convex hull composed of the point of interest and its neighborhood have the same value. Empirical experiments on different synthetic and real datasets have showed that these values were not the same when varying *points_of_interest* or *n_neighbors*. You have probably observed the same. **Why does it happen?** Because the dataset has its own nuances in its topology, which cause the local accuracies, considering only the neighborhoods of the points of interest, to be different.\n",
    "\n",
    "At this point, we propose the **accuracy degradation profile (ADP)**, a method to evaluate the **robustness** of machine learning models on datasets by iteratively **reducing the test set size** and analyzing the impact on accuracy. ADP points out on each portions (of the reduced dataset) the mean accuracy over each sample considering its neighborhoods falls dows over a defined threshold. APD is clearly understandable by an example.\n",
    "\n",
    "Let us perform ADP over the selected dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform accuracy degradation profile (ADP)\n",
    "results = accuracy_degradation_profile(X_test, \n",
    "                                    y_test, \n",
    "                                    y_pred, \n",
    "                                    n_neighbors = 200,\n",
    "                                    step_size = 0.05,\n",
    "                                    )\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the ADP profile matrix, column-by-column:\n",
    "\n",
    "- **size_factor** is the proportion of the test set that is being considered. If e.g. **size_factor** = 0.95, all samples of the test set are considering just the 95% nearest neighboors for accuracy calculation.\n",
    "\n",
    "- **above_threshold** is the number of samples with accuracy above the *threshold*. The variable *threshold* is the minimum acceptable accuracy, calculated as *baseline_accuracy* * *threshold_percentual*, with the second term being the threshold percentage for accuracy degradation. If e.g. **above_threshold** = 30, then 30 samples of the test set showed its accuracy (calculated over its neighboorhood) above the minimum acceptable accuracy. Default value for *threshold_percentual*: 0.95.\n",
    "\n",
    "- **percent_above** is the proportion of samples with accuracy above the *threshold*. If e.g. **percent_above** = 1.0, then 100% of samples of the test set showed its accuracy (calculated over its neighboorhood) above the minimum acceptable accuracy.\n",
    "\n",
    "- **decision** is the result of *above_threshold* compared with *above_percentual*, with the second term being the percentage of samples required to be above the threshold to avoid degradation. If *above_threshold* is greater than or equal to *above_percentual*, then there is no accuracy degradation (marked as 'OK').  If *above_threshold* is smaller than *above_percentual*, then there is accuracy degradation (marked as 'acc degrad!'). Default value for *above_percentual*: 0.90."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**:\n",
    "\n",
    "Every time you see 'acc degrad!' on the ADP matrix, **watch out** (!!!): *the mean accuracy over the convex hulls (samples and its neighboorhood) are not superior than than the minimum acceptable accuracy*. It may be valuable to look deeper on the topology of the dataset to check what is actually happening with your classifier applied to the reduced dataset. Even if your dataset is multidimensional, you may use dimensionality reduction to look at it in 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **accuracy degradation factor** (ADF) is the first **size_factor** on which an accuracy degradation occurs. ADF ranges from 0.0 to 1.0. As closer it is to 1.0, **less robust** is the model to dataset shifts considering the ADP methodology. As closer it is to 0.0, **more robust** is the model to dataset shifts considering the ADP methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = accuracy_degradation_factor(pd.DataFrame(results.data))\n",
    "adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ADP and ADF\n",
    "plot_adp_and_adf(results.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now apply ADP and ADF to **real datasets:**\n",
    "\n",
    "(You can import your own dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from holisticai.datasets import load_dataset\n",
    "\n",
    "# Choose any of the following datasets:\n",
    "# 'adult'\n",
    "# 'law_school'\n",
    "# 'student_multiclass'\n",
    "# 'us_crime_multiclass'\n",
    "# 'clinical_records'\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset('clinical_records')\n",
    "\n",
    "# Shrink the dataset\n",
    "n_rows = 1000 # Select only the first n rows\n",
    "# n_rows = dataset.data.shape[0] # Select all rows\n",
    "\n",
    "X = dataset['X'].iloc[:n_rows,:]\n",
    "y = dataset['y'].iloc[:n_rows]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test, test_indices = pre_process_data(X, y, test_size = 0.3, random_state = random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier over the data\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test.values)\n",
    "\n",
    "# Accuracy over the entire test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy test set: {accuracy_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform accuracy degradation profile (ADP)\n",
    "results = accuracy_degradation_profile(X_test, \n",
    "                                    y_test, \n",
    "                                    y_pred, \n",
    "                                    n_neighbors = 50,\n",
    "                                    step_size = 0.01,\n",
    "                                    )\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy over the test set\n",
    "accuracy_test = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy test set: {accuracy_test:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform accuracy degradation profile (ADP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = accuracy_degradation_factor(pd.DataFrame(results.data))\n",
    "adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ADP and ADF\n",
    "plot_adp_and_adf(results.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to visualize the projections on a 2D plot of your multidimensional dataset.\n",
    "\n",
    "First, let us choose the 2 features to plot using the *feature importance* criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_import = pd.DataFrame(clf.feature_importances_, index = X.columns).sort_values(by=0, ascending=False).rename(columns={0: 'Feature Importance'})\n",
    "feat_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may choose among **float** variables:\n",
    "\n",
    "(float variables may offer a better visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_import.loc[X.select_dtypes(include=['float64']).columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose features to plot\n",
    "# # features_to_plot = ['age', 'capital-gain'] # manually select\n",
    "# # features_to_plot = feat_import.sample(n=2).index # randomly select\n",
    "# features_to_plot = feat_import.index[:2] # select the most important ones\n",
    "# X = X[features_to_plot]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can optionally choose the most relevant PCs of PCA as axis to plot\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X)\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "features_to_plot = ['PC1', 'PC2']\n",
    "X = pca_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign data structures\n",
    "X_train, X_test, y_train, y_test, test_indices = pre_process_data(X, y, test_size = 0.3, random_state = random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D classification data\n",
    "plot_2d(X, y, features_to_plot = features_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlight the test set on the plot\n",
    "plot_2d(X, y, highlight_group=test_indices, features_to_plot = features_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot test set with index\n",
    "plot_2d(X,\n",
    "        y,\n",
    "        highlight_group=test_indices,\n",
    "        show_just_group=True,\n",
    "        features_to_plot = features_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot y_test and y_pred simultaneously\n",
    "# It maybe necessary to adjust the 'vertical_offset' parameter\n",
    "plot_label_and_prediction(X_test,\n",
    "                          y_test,\n",
    "                          y_pred,\n",
    "                          vertical_offset=0.2,\n",
    "                          features_to_plot = features_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the size of the neighborhood\n",
    "n_neighbors = 10\n",
    "\n",
    "# Choose the points of interest\n",
    "points_of_interest = test_indices[:2]\n",
    "\n",
    "# Plot points of interest and its neighborhoods\n",
    "plot_neighborhood(X,\n",
    "                  y_test,\n",
    "                  y_pred,\n",
    "                  n_neighbors,\n",
    "                  points_of_interest = test_indices,\n",
    "                  vertical_offset = 0.2,\n",
    "                  features_to_plot = features_to_plot,\n",
    "                  test_indices = test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
