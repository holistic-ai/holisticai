{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, sys\n",
        "\n",
        "dirpath = os.path.join( '/'.join(os.getcwd().split(\"/\")[:-5]), 'src')\n",
        "sys.path.insert(0, dirpath)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AsyvVC7QPrrX"
      },
      "source": [
        "# **Bias measuring and mitigation in classification tasks**\n",
        "\n",
        "With the increasing use of machine learning models in different areas, it has become important to address the bias problem in these models. This issue can appear in different aspects such as racial, gender or socioeconomic biases leading to unfair outcomes in decision-making processes, for instance, in classification tasks, where models are trained to classify data into different categories.\n",
        "\n",
        "There are various techniques to measure bias in classification models such as Equalized Odds, Demographic Parity or Opportunity Equality for instance, which quantify the differences in the model's performance across different subgroups based on sensitive attributes such as gender or race.\n",
        "\n",
        "Once bias is detected, we can employ different techniques to mitigate it. These methods can be grouped into three categories: Pre-processing, in-processing and post-processing methods. Pre-processing techniques are used to adjust the training data to remove bias, while in-processing methods are applied to build robust models against bias. Finally, post-processing techniques are used to adjust the model's predictions to remove bias.\n",
        "\n",
        "Through this tutorial, we pretend to present you tools which can be easily applied to measure and mitigate the presence of bias in classification models.\n",
        "\n",
        "We will follow the traditional outline for this tutorial:\n",
        "\n",
        "1. Data loading and packages installation\n",
        "2. Dataset preprocessing\n",
        "3. Data analysis\n",
        "4. Model training\n",
        "5. Bias measuring\n",
        "6. Bias mitigation\n",
        "7. Results comparison"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2QysNRo5pwUx"
      },
      "source": [
        "## **1. Data loading and packages installation**\n",
        "\n",
        "First of all, we need to import the required packages to perform our bias analysis and mitigation. You will need to have the `holisticai` package installed on your system, remember that you can install it by running: \n",
        "```bash\n",
        "!pip install holisticai[all]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb0SfHEYJfNo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **2. Dataset preprocessing**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3qjV2r1KPyBE"
      },
      "source": [
        "The dataset that we will use is the \"Adult\" dataset from the UCI Machine Learning Repository, this is a publicly available dataset that contains information about age, education, marital status, race and gender of individuals from the United States. The objective is to predict whether an individual's income will be above or below $50K per year.\n",
        "\n",
        "Source: [UCI ML repository](https://archive.ics.uci.edu/ml/datasets/adult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from holisticai.datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('adult')\n",
        "dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gHGiItN2qGP3"
      },
      "source": [
        "Now that we have a clean dataset we can start defining the training and testing sets."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tie2WueYP8de"
      },
      "source": [
        "## **3. Data analysis**\n",
        "\n",
        "Since this function already return the protected groups, we can start with the data analysis step. In this step, we will analyze the distribution of the data of the protected groups by using the `group_pie_plot` function. This function will plot the distribution of the data of the protected groups in a pie chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "sJeLw7QlQAhs",
        "outputId": "8606cab4-63d9-4204-df2a-de93bebf0bb9"
      },
      "outputs": [],
      "source": [
        "from holisticai.plots.bias import group_pie_plot, frequency_plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "group_a = dataset['p_attr']['group_a']\n",
        "group_b = dataset['p_attr']['group_b']\n",
        "y = dataset['y']\n",
        "\n",
        "fig,axs = plt.subplots(1, 2, figsize=(15,5))\n",
        "group_pie_plot(group_a, ax=axs[0], title='Group Pie Plot')\n",
        "frequency_plot(group_a, dataset['y'], ax=axs[1], title='Frequency Plot')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3OudU9COQKul"
      },
      "source": [
        "The previous graphs show us that the proportion of male (labelled as *False*) is high compared to the female.\n",
        "\n",
        "Let's plot the columns with the highest correlations with respect to the target variable. To do this, we will use the `correlation_matrix_plot` function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from holisticai.plots.bias import correlation_matrix_plot\n",
        "\n",
        "df = pd.concat([dataset['x'], dataset['y']], axis=1)\n",
        "\n",
        "correlation_matrix_plot(df, target_feature='y', n_features=10)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kH2sluSNQDEN"
      },
      "source": [
        "Now that we have our protected groups, we will define the input/output sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1FVWNLPJ86z"
      },
      "outputs": [],
      "source": [
        "dataset_split = dataset.train_test_split(test_size=0.3)\n",
        "dataset_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train a model, do not forget to standard scale data\n",
        "train = dataset_split['train']\n",
        "test = dataset_split['test']\n",
        "\n",
        "group_a_test = test['p_attr']['group_a']\n",
        "group_b_test = test['p_attr']['group_b']\n",
        "group_a_train = train['p_attr']['group_a']\n",
        "group_b_train = train['p_attr']['group_b']"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "L21_e4JxKHV3"
      },
      "source": [
        "## **4. Model training**\n",
        "\n",
        "Once we have defined the input and output sets, we can train a model as usual. Since the protected groups were separated from the dataset previously we do not need to take care of that in this opportunity, but do not forget to separate the protected attributes from the dataset, so that the model does not have any influence from these attributes in its training process. For the training process, we will use a traditional pipeline, we will fit and re-scale the training data, and then we will use the data to train a \"Logistic regression\" model and once the model has been trained, we can use its predictions to calculate the fairness metrics of the it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFq4uGn6KE4x"
      },
      "outputs": [],
      "source": [
        "# sklearn imports\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from holisticai.pipeline import Pipeline\n",
        "\n",
        "# train a model, do not forget to standard scale data\n",
        "train = dataset_split['train']\n",
        "test = dataset_split['test']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_t = scaler.fit_transform(train['x'])\n",
        "model = LogisticRegression(random_state=42, max_iter=500)\n",
        "model.fit(X_train_t, train['y'])\n",
        "X_test_t = scaler.transform(test['x'])\n",
        "y_pred = model.predict(X_test_t)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3QJ4YkOQQdlw"
      },
      "source": [
        "## **5. Bias measuring**\n",
        "\n",
        "The fairness of the model can be calculated using the predictions of the model and the protected groups defined previously. The `holisticai` contains a module that calculates a set of metrics useful in evaluating the fairness of algorithmic decisions. For our case, we will use the `classification_bias_metrics6` function which allows us to select which metrics we want to calculate, if `equal_outcome`, `equal_opportunity` or `both`, where equal_outcome shows how disadvantaged groups are treated by the model and equal_opportunity shows if all the groups have the same opportunities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xwX__NvQfoc"
      },
      "outputs": [],
      "source": [
        "from holisticai.bias.metrics import classification_bias_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "yC86XKf2Qg8c",
        "outputId": "d0be4b0b-d63a-46c0-8082-a8f6d4db0a03"
      },
      "outputs": [],
      "source": [
        "df = classification_bias_metrics(\n",
        "    group_a_test,\n",
        "    group_b_test,\n",
        "    y_pred,\n",
        "    test['y'],\n",
        "    metric_type='both'\n",
        ")\n",
        "y_baseline = y_pred.copy()\n",
        "df_baseline=df.copy()\n",
        "df_baseline"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o6uJIcLhQpIx"
      },
      "source": [
        "For example:\n",
        "\n",
        "*   **Statistical parity:** Computes the difference in success rates between the protected groups. Values below 0 are considered unfair towards `group_a` while values above 0 are considered unfair towards `group_b`, the range (-0.1, 0.1) is considered acceptable.\n",
        "*   **Disparate Impact:** Shows the ratio of success rates between the protected groups for a certain quantile. Values below 1 are unfair towards `group_a`. Values above 1 are unfair towards `group_b`. The range (0.8, 1.2) is considered acceptable.\n",
        "*   **Four Fifths:** Computes the ratio of success rates between the protected groups. Values below 1 are considered unfair while a range between (0.8, 1) is considered acceptable.\n",
        "*   **Cohen D:** Computes the normalised statistical parity between the protected groups. Values below 0 are considered unfair towards `group_a` while values above 0 are considered unfair towards `group_b`.\n",
        "*   **Equality of opportunity difference:** Computes the difference in true positive rates between the protected groups. Values below 0 are considered unfair towards `group_a` while values above 0 are considered unfair towards `group_b`.\n",
        "*   **False positive rate difference:** Computes the difference in false positive rates between the protected groups, negative values indicating bias against `group_a` while positive values indicating bias against `group_b`.\n",
        "*   **Average Odds Difference:** Computes the difference in average odds between the protected groups, negative values indicating bias against `group_a` while positive values indicating bias against `group_b`, a range between (-0.1, 0.1) is considered acceptable.\n",
        "*   **Accuracy Difference:** Computes the difference in accuracy of predictions for the protected groups, positive values show bias against `group_b` while negative values show bias against `group_a`.\n",
        "\n",
        "\n",
        "Source: [*HolisticAI docs*](https://holisticai.readthedocs.io/en/latest/metrics.html#binary-classification)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3gx__DiQKeiA"
      },
      "source": [
        "## **6. Bias mitigation**\n",
        "### **Adversarial debiasing: Inprocessing method**\n",
        "\n",
        "Now that we could observe that the model metrics are far away from the desired values, we need to apply some kind of strategy to mitigate the bias present in the model.\n",
        "\n",
        "Exists different kinds of strategies, and the literature has divided them into three categories: *Pre-processing*, *in-processing* and *post-processing* methods. The `holisticai` library possesses different algorithms from these categories for bias mitigation. An interesting feature is that all of them are compatible with the `Scikit-learn` package, so that, if you are familiar with this package, you will not have problems using the library. As you will see later, its implementation can be done following the traditional way, or by applying the pipeline. \n",
        "\n",
        "For this opportunity, we will implement the \"Adversarial debiasing\" method which is an in-processing technique that learns a classifier to maximize prediction accuracy and simultaneously reduce an adversary's ability to determine the protected attribute from the predictions."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sUUZTGK1Mfjv"
      },
      "source": [
        "### **Traditional implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from holisticai.bias.mitigation import AdversarialDebiasing\n",
        "np.random.seed(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "0N53PlHGLWw9",
        "outputId": "3a4a1699-6e10-4d4e-f9cc-92c8985c4115"
      },
      "outputs": [],
      "source": [
        "inprocessing_model = AdversarialDebiasing(\n",
        "        features_dim=train['x'].shape[1],\n",
        "        batch_size=32,\n",
        "        hidden_size=64,\n",
        "        adversary_loss_weight=0.1,\n",
        "        verbose=1,\n",
        "        use_debias=True,\n",
        "        seed=42,\n",
        "    ).transform_estimator()\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(train['x'])\n",
        "\n",
        "inprocessing_model.fit(X_train, train['y'], group_a_train, group_b_train)\n",
        "\n",
        "\n",
        "X_test = scaler.transform(test['x'])\n",
        "y_pred = inprocessing_model.predict(X_test)\n",
        "\n",
        "df = classification_bias_metrics(\n",
        "    group_a_test,\n",
        "    group_b_test,\n",
        "    y_pred,\n",
        "    test['y'],\n",
        "    metric_type='both'\n",
        ")\n",
        "df_adv_deb = df.copy()\n",
        "df_adv_deb"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZnbetpsKhxd"
      },
      "source": [
        "### **Pipeline implementation**\n",
        "\n",
        "Now that we could see how this method is implemented traditionally, let's try implementing it by using the Scikit-learn Pipeline! Notice that we shouldn't have variations in the metrics for the previous implementation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "x4vl8cM1K6K2",
        "outputId": "94e74117-63c4-4173-b793-c8ec531a4057"
      },
      "outputs": [],
      "source": [
        "inprocessing_model = AdversarialDebiasing(\n",
        "        features_dim=X.shape[1],\n",
        "        batch_size=32,\n",
        "        hidden_size=64,\n",
        "        adversary_loss_weight=0.1,\n",
        "        verbose=1,\n",
        "        use_debias=True,\n",
        "        seed=42,\n",
        "    ).transform_estimator()\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    steps=[\n",
        "        ('scalar', StandardScaler()),\n",
        "        (\"bm_inprocessing\", inprocessing_model),\n",
        "    ]\n",
        ")\n",
        "\n",
        "X, y, group_a, group_b = train_data\n",
        "fit_params = {\n",
        "    \"bm__group_a\": group_a, \n",
        "    \"bm__group_b\": group_b\n",
        "}\n",
        "\n",
        "pipeline.fit(X, y, **fit_params)\n",
        "\n",
        "X, y, group_a, group_b = test_data\n",
        "predict_params = {\n",
        "    \"bm__group_a\": group_a,\n",
        "    \"bm__group_b\": group_b,\n",
        "}\n",
        "y_pred = pipeline.predict(X, **predict_params)\n",
        "df = classification_bias_metrics(\n",
        "    group_a,\n",
        "    group_b,\n",
        "    y_pred,\n",
        "    y,\n",
        "    metric_type='both'\n",
        ")\n",
        "df_adv_deb_w_p = df.copy()\n",
        "df_adv_deb_w_p"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4PK-M6o5LvYs"
      },
      "source": [
        "## **7. Results comparison**\n",
        "Now that we could observe how is the implementation of the mitigator in the model, we will compare the results between the baseline and the implementations with the mitigator to analyse how the metrics have changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "pNVKX4kKLpsz",
        "outputId": "ed3aa496-0cac-4625-d5dc-f348bb9b56fb"
      },
      "outputs": [],
      "source": [
        "result = pd.concat([df_baseline, df_adv_deb, df_adv_deb_w_p], axis=1).iloc[:, [0,2,4,1]]\n",
        "result.columns = ['Baseline','Mitigator without pipeline',\"Mitigator with pipeline\", 'Reference']\n",
        "result"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RM1iEX7mR7ZH"
      },
      "source": [
        "From the previous chart, we can see that although the actual metrics are still far from the ideal values, an improvement is obtained by applying this method in the dataset, compared with our baseline. \n",
        "\n",
        "Notice that we can analyse how these values are affected by varying the hyperparameters of the method. For example, this method allows us to set different hyperparameters such us the hidden size, the batch size, the adversarial loss weight and so on.\n",
        "\n",
        "In this opportunity we will try with different values of the adversarial loss weight:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3X1keA2QORTA"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(train_data, test_data, adversary_loss_weight, metrics_cols):\n",
        "  inprocessing_model = AdversarialDebiasing(\n",
        "        features_dim=train_data[0].shape[1],\n",
        "        batch_size=32,\n",
        "        hidden_size=64,\n",
        "        adversary_loss_weight=adversary_loss_weight,\n",
        "        verbose=1,\n",
        "        use_debias=True,\n",
        "        seed=42,\n",
        "    ).transform_estimator()\n",
        "\n",
        "  pipeline = Pipeline(\n",
        "      steps=[\n",
        "          ('scalar', StandardScaler()),\n",
        "          (\"bm_inprocessing\", inprocessing_model),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  X, y, group_a, group_b = train_data\n",
        "  fit_params = {\n",
        "      \"bm__group_a\": group_a, \n",
        "      \"bm__group_b\": group_b\n",
        "  }\n",
        "\n",
        "  pipeline.fit(X, y, **fit_params)\n",
        "\n",
        "  X, y, group_a, group_b = test_data\n",
        "  predict_params = {\n",
        "      \"bm__group_a\": group_a,\n",
        "      \"bm__group_b\": group_b,\n",
        "  }\n",
        "  y_pred = pipeline.predict(X, **predict_params)\n",
        "  df = classification_bias_metrics(\n",
        "      group_a,\n",
        "      group_b,\n",
        "      y_pred,\n",
        "      y,\n",
        "      metric_type='both'\n",
        "  )\n",
        "  # Calculate metrics\n",
        "  metrics = [\n",
        "      adversary_loss_weight, \n",
        "  ]\n",
        "  metrics.extend([df['Value'].loc[col] for col in metrics_cols])\n",
        "  return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "4eMalN1YSXZX",
        "outputId": "b2e19cf0-b87b-4eba-a7fd-ed5eb663054b"
      },
      "outputs": [],
      "source": [
        "metrics_val = []\n",
        "metrics_cols = ['Statistical Parity', 'Disparate Impact', 'Four Fifths Rule']\n",
        "\n",
        "for adversary_loss_weight in np.linspace(0, 1, 5):\n",
        "  metrics_val.append(calculate_metrics(train_data, test_data, adversary_loss_weight, metrics_cols))\n",
        "\n",
        "metrics = ['adversary_loss_weight']\n",
        "metrics.extend(metrics_cols)\n",
        "df_metrics = pd.DataFrame(metrics_val, columns=metrics)\n",
        "df_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "p_EINWLRSl_Y",
        "outputId": "680de6da-2382-4c42-a607-d80917f094f6"
      },
      "outputs": [],
      "source": [
        "x_axis = df_metrics.adversary_loss_weight\n",
        "plt.figure(figsize=(25,8))\n",
        "plt.subplot(121)\n",
        "plt.plot(x_axis, df_metrics['Disparate Impact'], 'o-', label='Disparate impact')\n",
        "plt.plot(x_axis, np.ones_like(df_metrics['Disparate Impact']), label='Reference')\n",
        "plt.legend(loc='upper left')\n",
        "plt.xlabel('Adversary loss weight')\n",
        "plt.subplot(122)\n",
        "plt.plot(x_axis, df_metrics['Statistical Parity'], 'o-', label='Statistical Parity')\n",
        "plt.plot(x_axis, np.zeros_like(df_metrics['Statistical Parity']), label='Reference')\n",
        "plt.legend(loc='upper left')\n",
        "plt.xlabel('Adversary loss weight')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "R-ZqEsuNl8O5"
      },
      "source": [
        "From the previous graphs, we can see that increasing this value too much will not improve our model in terms of fairness, therefore we need to maintain this value as lower as possible. Let's try with a smaller range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "Gs-CSnjY4f8S",
        "outputId": "5e7b2826-c751-4ace-9a4b-bab0a1938bac"
      },
      "outputs": [],
      "source": [
        "metrics_val = []\n",
        "metrics_cols = ['Statistical Parity', 'Disparate Impact', 'Four Fifths Rule']\n",
        "\n",
        "for adversary_loss_weight in np.linspace(0, 0.3, 5):\n",
        "  metrics_val.append(calculate_metrics(train_data, test_data, adversary_loss_weight, metrics_cols))\n",
        "\n",
        "metrics = ['adversary_loss_weight']\n",
        "metrics.extend(metrics_cols)\n",
        "df_metrics = pd.DataFrame(metrics_val, columns=metrics)\n",
        "df_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        },
        "id": "XhVXTZN55h7A",
        "outputId": "83f7160f-bfe8-4553-901d-c6da475dcc22"
      },
      "outputs": [],
      "source": [
        "x_axis = df_metrics.adversary_loss_weight\n",
        "plt.figure(figsize=(25,8))\n",
        "plt.subplot(121)\n",
        "plt.plot(x_axis, df_metrics['Disparate Impact'], 'o-', label='Disparate impact')\n",
        "plt.plot(x_axis, np.ones_like(df_metrics['Disparate Impact']), label='Reference')\n",
        "plt.legend(loc='upper left')\n",
        "plt.xlabel('Adversary loss weight')\n",
        "plt.subplot(122)\n",
        "plt.plot(x_axis, df_metrics['Statistical Parity'], 'o-', label='Statistical Parity')\n",
        "plt.plot(x_axis, np.zeros_like(df_metrics['Statistical Parity']), label='Reference')\n",
        "plt.legend(loc='upper left')\n",
        "plt.xlabel('Adversary loss weight')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y5trf3OTg-d"
      },
      "source": [
        "Now we can see that although we obtain a similar behaviour as before, for this case a fairer model is obtained with an adversary loss weight value close to 0.3. Thereby, the selection of the model parameters will depend on our main objective, whether we are looking for fairness or accuracy."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lpmihIp_T1Oq"
      },
      "source": [
        "## **Summary**\n",
        "Through this tutorial we could present to the user how the `holisticai` library can be easily used to measure the bias present in classification models by the application of the `classification_bias_metrics` function, which returns the calculated values for different metrics and their references respectively. \n",
        "\n",
        "In addition, we shown how to mitigate the bias by applying some methods, for this opportunity we applied the \"Adversarial debiasing\" technique to train a fairness model, this is a in-processing method that leads to a fair classifier as the predictions cannot carry any group discrimination information that the adversary can exploit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMZ4ZNsuS6sJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
