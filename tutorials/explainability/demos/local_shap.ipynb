{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 1. Regression"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1.1. Load Data and Train Model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# SHAP Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#from sklearn.linear_model import LinearRegression\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from holisticai.datasets import load_dataset\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "dataset = load_dataset('us_crime')\n",
                "dataset = dataset.train_test_split(test_size=1000, random_state=42)\n",
                "train = dataset['train']\n",
                "test = dataset['test']\n",
                "\n",
                "model = DecisionTreeRegressor()\n",
                "model.fit(train['X'], train['y'])\n",
                "\n",
                "mean_squared_error(test['y'], model.predict(test['X']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Create a Custom Feature Importance Calculator"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.utils import RegressionProxy\n",
                "from holisticai.utils.feature_importances import compute_shap_feature_importance\n",
                "from holisticai.utils.inspection import compute_partial_dependence\n",
                "\n",
                "X = test['X']\n",
                "proxy = RegressionProxy(predict=model.predict)\n",
                "\n",
                "local_importances = compute_shap_feature_importance(X=X, proxy=proxy)\n",
                "local_conditional_importances = local_importances.conditional()\n",
                "\n",
                "importances = local_importances.to_global()\n",
                "conditional_importances = local_conditional_importances.to_global()\n",
                "\n",
                "ranked_importances = importances.top_alpha(0.8)\n",
                "\n",
                "partial_dependencies = compute_partial_dependence(X, features=ranked_importances.feature_names, proxy=proxy)\n",
                "\n",
                "y_pred = proxy.predict(X)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.explainability.metrics import regression_explainability_metrics\n",
                "\n",
                "regression_explainability_metrics(importances=importances, \n",
                "                                partial_dependencies=partial_dependencies, \n",
                "                                conditional_importances=conditional_importances, \n",
                "                                local_importances=local_importances)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.explainability.plots import plot_feature_importance\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "fig,ax = plt.subplots(1,1, figsize=(5,10))\n",
                "plot_feature_importance(importances, top_n=50, ax=ax)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.explainability.plots import plot_partial_dependence\n",
                "\n",
                "plot_partial_dependence(partial_dependencies, ranked_importances, subplots=(4,3), figsize=(8, 8))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.explainability.plots import plot_local_importance_distribution\n",
                "\n",
                "plot_local_importance_distribution(local_importances)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.explainability.plots import plot_predictions_vs_interpretability\n",
                "\n",
                "plot_predictions_vs_interpretability(y_pred, local_importances)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 3. Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.utils import BinaryClassificationProxy\n",
                "from holisticai.utils.feature_importances import compute_shap_feature_importance\n",
                "from holisticai.utils.inspection import compute_partial_dependence\n",
                "from numpy.random import RandomState\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from holisticai.datasets import load_dataset\n",
                "from sklearn.metrics import accuracy_score\n",
                "import matplotlib.pyplot as plt\n",
                "import numpy as np\n",
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset = load_dataset(dataset_name=\"adult\")\n",
                "dataset = dataset.train_test_split(test_size=2000, random_state=42, stratify=dataset['y'])\n",
                "train = dataset['train']\n",
                "test = dataset['test']\n",
                "\n",
                "model = LogisticRegression()\n",
                "model.fit(train['X'], train['y'])\n",
                "\n",
                "accuracy_score(test['y'], model.predict(test['X']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "X = test['X']\n",
                "proxy = BinaryClassificationProxy(predict=model.predict, predict_proba=model.predict_proba, classes=model.classes_)\n",
                "\n",
                "local_importances = compute_shap_feature_importance(X=X, proxy=proxy)\n",
                "local_conditional_importances = local_importances.conditional()\n",
                "importances = local_importances.to_global()\n",
                "conditional_importances = local_conditional_importances.to_global()\n",
                "\n",
                "ranked_importances = importances.top_alpha(0.8)\n",
                "\n",
                "partial_dependencies = compute_partial_dependence(train['X'], features=ranked_importances.feature_names, proxy=proxy)\n",
                "\n",
                "y_pred = proxy.predict(X)\n",
                "y_score = proxy.predict_proba(X)[:,1]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.explainability.metrics import classification_explainability_metrics\n",
                "\n",
                "classification_explainability_metrics(importances, partial_dependencies, conditional_importances, local_importances=local_importances)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_local_importance_distribution(local_importances)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plot_predictions_vs_interpretability(y_score, local_importances)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.explainability.plots import plot_feature_importance\n",
                "\n",
                "plot_feature_importance(importances, top_n=8)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.explainability.plots import plot_partial_dependence\n",
                "\n",
                "class_index = 0\n",
                "plot_partial_dependence(partial_dependencies, ranked_importances, class_idx=class_index)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multi Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.ensemble import GradientBoostingClassifier\n",
                "\n",
                "from holisticai.datasets import load_dataset\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "dataset = load_dataset(dataset_name=\"student_multiclass\")\n",
                "dataset = dataset.train_test_split(test_size=200, random_state=42, stratify=dataset['y'])   \n",
                "train = dataset['train']\n",
                "test = dataset['test']\n",
                "\n",
                "model = GradientBoostingClassifier()\n",
                "model.fit(train['X'], train['y'])\n",
                "\n",
                "accuracy_score(test['y'], model.predict(test['X']))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.utils import MultiClassificationProxy\n",
                "from holisticai.utils.feature_importances import compute_shap_feature_importance\n",
                "from holisticai.utils.inspection import compute_partial_dependence\n",
                "\n",
                "proxy = MultiClassificationProxy(predict=model.predict, predict_proba=model.predict_proba, classes=model.classes_)\n",
                "\n",
                "local_importances = compute_shap_feature_importance(X=train['X'], y=train['y'], proxy=proxy, max_samples=200)\n",
                "local_conditional_importances = local_importances.conditional()\n",
                "\n",
                "importances = local_importances.to_global()\n",
                "conditional_importances = local_conditional_importances.to_global()\n",
                "\n",
                "ranked_importances = importances.top_alpha(0.8)\n",
                "\n",
                "partial_dependencies = compute_partial_dependence(test['X'], features=ranked_importances.feature_names, proxy=proxy)\n",
                "\n",
                "y_pred = proxy.predict(test['X'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.explainability.metrics import multiclass_explainability_metrics\n",
                "\n",
                "multiclass_explainability_metrics(importances, partial_dependencies, conditional_importances, test['X'], y_pred, local_importances)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.explainability.plots import plot_feature_importance\n",
                "\n",
                "plot_feature_importance(importances, top_n=30)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from holisticai.explainability.plots import plot_partial_dependence\n",
                "\n",
                "class_idx = 0 #1\n",
                "plot_partial_dependence(partial_dependencies, ranked_importances, subplots=(3,4), figsize=(10,8), class_idx=class_idx)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
