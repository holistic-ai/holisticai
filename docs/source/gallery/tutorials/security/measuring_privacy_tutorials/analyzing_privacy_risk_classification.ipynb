{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing privacy risk score for classification models\n",
    "\n",
    "In this notebook, we will analyze the privacy risks of a classification model. We will use the [bank marketing dataset](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing) from the UCI Machine Learning Repository. This dataset is related to direct marketing campaigns of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit.\n",
    "\n",
    "This is useful for understanding some privacy concepts related to membership inference attacks in machine learning models. To do this, we will use the shadow training technique commonly used in membership inference attacks and then we will evaluate the privacy risk score of a target model concerning the dataset. Then, we will analyze the privacy risk score by changing the model architecture to observe the impact of the model choice on the privacy risk score.\n",
    "\n",
    "Let's start by importing the necessary libraries and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from holisticai.datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from holisticai.security.metrics import privacy_risk_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the privacy risk score, we will use the shadow training technique. This technique consists of training a shadow model to mimic the target model. \n",
    "\n",
    "For our purposes, we will assume that the adversary has access to part of the training dataset and full access to the testing dataset. The adversary will use this dataset to train the shadow model, which is a model that tries to mimic the target model.\n",
    "\n",
    "To do this, once we have our training and testing sets, we will split the training dataset into two parts: the target dataset, which will be used to calculate the probabilities from the target model; and the shadow dataset, which will be used to train the target model. We will use the following proportions: 60% for the target dataset and 40% for the shadow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('bank_marketing', protected_attribute='marital', preprocessed=True)\n",
    "train_test = dataset.train_test_split(test_size=0.2, random_state=42)\n",
    "train = train_test['train']\n",
    "target_shadow = train.train_test_split(test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 36168\n",
      "Test set size: 9043\n",
      "Target Model Training set size: 21700\n",
      "Shadow Model Training set size: 14468\n"
     ]
    }
   ],
   "source": [
    "X_target_train = target_shadow['train']['X']\n",
    "y_target_train = target_shadow['train']['y']\n",
    "X_shadow_train = target_shadow['test']['X']\n",
    "y_shadow_train = target_shadow['test']['y']\n",
    "\n",
    "X_test = train_test['test']['X']\n",
    "y_test = train_test['test']['y']\n",
    "\n",
    "X_train = train['X']\n",
    "y_train = train['y']\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])\n",
    "\n",
    "print(\"Target Model Training set size:\", X_target_train.shape[0])\n",
    "print(\"Shadow Model Training set size:\", X_shadow_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models training\n",
    "Now, we can train the target model. In this case, we will use a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Model Performance:\n",
      "Accuracy: 0.9027977441114674\n"
     ]
    }
   ],
   "source": [
    "# Train the Target Model\n",
    "target_model = RandomForestClassifier(random_state=42)\n",
    "target_model.fit(X_train, y_train)\n",
    "y_target_pred = target_model.predict(X_test)\n",
    "print(\"Target Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_target_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the same for the shadow model. Just remember that the shadow model will be trained with the shadow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shadow Model Performance:\n",
      "Accuracy: 0.8974897710936636\n"
     ]
    }
   ],
   "source": [
    "# Train the Shadow Model\n",
    "shadow_model = RandomForestClassifier(random_state=42)\n",
    "shadow_model.fit(X_shadow_train, y_shadow_train)\n",
    "y_shadow_pred = shadow_model.predict(X_test)\n",
    "print(\"\\nShadow Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_shadow_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Privacy risk score calculation\n",
    "\n",
    "Once we have the target and shadow models, we can calculate the privacy risk score for each sample in the dataset. With this metric, we can estimate the probability of a sample belonging to the training dataset of the target model. In this way we can identify which samples present a higher risk of privacy leakage. To read more about the privacy risk score, you can check the original [paper](https://arxiv.org/abs/2003.10595).\n",
    "\n",
    "To measure the privacy risk score, we first need to calculate the probability of the training and testing samples belonging to the target and shadow models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_train_probs = shadow_model.predict_proba(X_shadow_train)\n",
    "shadow_test_probs = shadow_model.predict_proba(X_test)\n",
    "target_train_probs = target_model.predict_proba(X_target_train)\n",
    "target_test_probs = target_model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the probabilities of the input samples belonging to the target and shadow model, we can then calculate the probability of the input samples belonging to the training set by using the `privacy_risk_score` function. This function receives as input tuples with the probabilities of the training and testing samples for the target and shadow models.\n",
    "\n",
    "We will first calculate the privacy risk score for the training dataset and then for the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Privacy Risk Score for train:  0.5572013708112826\n",
      "Mean Privacy Risk Score for test:  0.4623020668629128\n"
     ]
    }
   ],
   "source": [
    "risk_score_train = privacy_risk_score((shadow_train_probs, y_shadow_train), (shadow_test_probs, y_test), (target_train_probs, y_target_train))\n",
    "print(\"Mean Privacy Risk Score for train: \", risk_score_train.mean())\n",
    "\n",
    "risk_score_test = privacy_risk_score((shadow_train_probs, y_shadow_train), (shadow_test_probs, y_test), (target_test_probs, y_test))\n",
    "print(\"Mean Privacy Risk Score for test: \", risk_score_test.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have calculated the privacy risk score of the training and testing set, we can then analyze the privacy risk of the target model by varing the threshold of the privacy risk score.\n",
    "\n",
    "To do this we will calculate the precision and recall of the privacy risk score for different thresholds. We will use the ROC curve function to calculate the false positive rate and true positive rate for different thresholds and then, we will use the calculated values to analyze the privacy risk of the target model at different thresholds.\n",
    "\n",
    "To create the ROC curve, we will use the `roc_curve` function from the `sklearn.metrics` module. This function receives as input the true labels and the predicted probabilities of the positive class. As labels, we know previously if the samples belong to the training or testing set, so we can use this information to create the labels for the ROC curve. On the other hand, the scores are the calculated privacy risk scores.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.concatenate((np.ones(len(risk_score_train)), np.zeros(len(risk_score_test))))\n",
    "scores = np.concatenate((risk_score_train, risk_score_test))\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels, scores, drop_intermediate=False)\n",
    "\n",
    "# Define threshold list and find meaningful thresholds\n",
    "threshold_list = [1., 0.9, 0.8, 0.7, 0.6, 0.5, 0.4]\n",
    "max_prob = max(risk_score_train.max(), risk_score_test.max())\n",
    "meaningful_thresholds = [threshold for threshold in threshold_list if threshold <= max_prob]\n",
    "\n",
    "# Find indices for meaningful thresholds\n",
    "indices = [np.argwhere(thresholds >= threshold)[-1][0] for threshold in meaningful_thresholds]\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision_list = tpr[indices] / (tpr[indices] + fpr[indices])\n",
    "recall_list = tpr[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the previous calculations allow us to analyze the privacy risk of the target model at different thresholds. We can do this by analyzing the precision and recall of the privacy risk score to extract insights about the privacy risk of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 1.0, Precision: 1.0, Recall: 0.0014285714285714286\n",
      "Threshold: 0.9, Precision: 0.9192934519839431, Recall: 0.03400921658986175\n",
      "Threshold: 0.8, Precision: 0.9192934519839431, Recall: 0.03400921658986175\n",
      "Threshold: 0.7, Precision: 0.9192934519839431, Recall: 0.03400921658986175\n",
      "Threshold: 0.6, Precision: 0.6012458562151735, Recall: 0.5895852534562211\n",
      "Threshold: 0.5, Precision: 0.5775898398535104, Recall: 0.7221658986175116\n",
      "Threshold: 0.4, Precision: 0.5480712814344799, Recall: 0.9387557603686636\n"
     ]
    }
   ],
   "source": [
    "for idx, threshold in enumerate(meaningful_thresholds):\n",
    "    print(f\"Threshold: {threshold}, Precision: {precision_list[idx]}, Recall: {recall_list[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these results, we can say the following:\n",
    "- If the privacy risk score is set above of 0.7, there are ~3% of the training set members that can be inferred correctly by the adversary with a ~92% of precision.\n",
    "- If we decrease the privacy risk score to 0.6, there are ~58% of the training set members that can be inferred correctly by the adversary with a ~60% of precision.\n",
    "- Surpringly, there is a very small percentage (0.1%) of the training set members that can be inferred correctly by the adversary with a ~100% of precision if the privacy risk score is set as 1. This is because the privacy risk score is calculated as the probability of the sample belonging to the training set, so if the privacy risk score is 1, the sample belongs to the training set with a probability of 1.\n",
    "- Finally, if the privacy risk score is set as 0.5, there are ~72% of the training set members that can be inferred correctly by the adversary with a ~57% of precision.\n",
    "\n",
    "We can see that for this particular model, we could have serious privacy risks since the adversary can infer a considerable percentage of the training set members with a considerable precision. This is a privacy risk that should be taken into account when deploying the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing the model architecture\n",
    "\n",
    "Now that we have analyzed the privacy risk of the model with a Random Forest classifier, we can analyze the privacy risk of the model with a different architecture. In this case, we will change the target model to a Logistic Regression classifier maintaining the same shadow model. We will repeat the same steps as before to analyze the privacy risk of the model with a different architecture.\n",
    "\n",
    "We will this analysis to observe the impact of the model choice on the privacy risk score.\n",
    "\n",
    "Let's train the target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Model Performance:\n",
      "Accuracy: 0.8906336392790003\n"
     ]
    }
   ],
   "source": [
    "# Train the Target Model\n",
    "target_model = LogisticRegression(random_state=42)\n",
    "target_model.fit(X_train, y_train)\n",
    "y_target_pred = target_model.predict(X_test)\n",
    "print(\"Target Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_target_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the accuracy of the model decreases, this change is not significant.\n",
    "\n",
    "Now, let's analyze the privacy risk of the target model built with a Logistic Regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Privacy Risk Score for train:  0.39248846087198713\n",
      "Mean Privacy Risk Score for test:  0.390332725228539 0.1548584345529434\n",
      "Threshold: 1.0, Precision: 0.5151883448597119, Recall: 0.002350230414746544\n",
      "Threshold: 0.9, Precision: 0.5036339840160396, Recall: 0.007741935483870968\n",
      "Threshold: 0.8, Precision: 0.5036339840160396, Recall: 0.007741935483870968\n",
      "Threshold: 0.7, Precision: 0.5036339840160396, Recall: 0.007741935483870968\n",
      "Threshold: 0.6, Precision: 0.5124566467179488, Recall: 0.02870967741935484\n",
      "Threshold: 0.5, Precision: 0.4995475163227362, Recall: 0.12230414746543779\n",
      "Threshold: 0.4, Precision: 0.5014759264484899, Recall: 0.7450691244239631\n"
     ]
    }
   ],
   "source": [
    "shadow_train_probs = shadow_model.predict_proba(X_shadow_train)\n",
    "shadow_test_probs = shadow_model.predict_proba(X_test)\n",
    "target_train_probs = target_model.predict_proba(X_target_train)\n",
    "target_test_probs = target_model.predict_proba(X_test)\n",
    "\n",
    "risk_score_train = privacy_risk_score((shadow_train_probs, y_shadow_train), (shadow_test_probs, y_test), (target_train_probs, y_target_train))\n",
    "print(\"Mean Privacy Risk Score for train: \", risk_score_train.mean())\n",
    "\n",
    "risk_score_test = privacy_risk_score((shadow_train_probs, y_shadow_train), (shadow_test_probs, y_test), (target_test_probs, y_test))\n",
    "print(\"Mean Privacy Risk Score for test: \", risk_score_test.mean(), risk_score_test.std())\n",
    "\n",
    "labels = np.concatenate((np.ones(len(risk_score_train)), np.zeros(len(risk_score_test))))\n",
    "scores = np.concatenate((risk_score_train, risk_score_test))\n",
    "\n",
    "# Compute ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(labels, scores, drop_intermediate=False)\n",
    "\n",
    "# Define threshold list and find meaningful thresholds\n",
    "threshold_list = [1., 0.9, 0.8, 0.7, 0.6, 0.5, 0.4]\n",
    "max_prob = max(risk_score_train.max(), risk_score_test.max())\n",
    "meaningful_thresholds = [threshold for threshold in threshold_list if threshold <= max_prob]\n",
    "\n",
    "# Find indices for meaningful thresholds\n",
    "indices = [np.argwhere(thresholds >= threshold)[-1][0] for threshold in meaningful_thresholds]\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision_list = tpr[indices] / (tpr[indices] + fpr[indices])\n",
    "recall_list = tpr[indices]\n",
    "\n",
    "for idx, threshold in enumerate(meaningful_thresholds):\n",
    "    print(f\"Threshold: {threshold}, Precision: {precision_list[idx]}, Recall: {recall_list[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these new results, we can observe and extract the following insights:\n",
    "- First, we could observe that although the target model has changed, the accuracy of the model is still high. But the real change is in the privacy risk score. For this case, we have a mean privacy risk score of 0.4, while for the Random Forest classifier, the mean privacy risk score was 0.55.\n",
    "- Next, we can see that the privacy risk score is lower for the Logistic Regression classifier than for the Random Forest classifier. This means that the adversary can infer fewer training set members with a lower precision for the Logistic Regression classifier than for the Random Forest classifier.\n",
    "- Finally, we can observe the same behavior for different thresholds. With an almost 2% of the training set members that can be inferred correctly by the adversary with a ~51% of precision for a privacy risk score of 0.6, while for the Random Forest classifier, this percentage was 58% with a ~60% of precision.\n",
    "\n",
    "From these results, we can see that the model choice has an impact on the privacy risk score. In this case, the Logistic Regression classifier has a lower privacy risk score than the Random Forest classifier. This is an important insight to consider when deploying a model, as the privacy risk score can vary depending on the model architecture.\n",
    "\n",
    "In addition, although the privacy risk score can be a useful metric to analyze the privacy risks of a model, it is important to consider other metrics and techniques to ensure the privacy of the data and the model. For example, the implementation of defenses against membership inference attacks, such as differential privacy or adversarial training, can help to mitigate the privacy risks of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have analyzed the privacy risk of a classification model using the shadow training technique. We have calculated the privacy risk score for the training and testing datasets and then we have analyzed the privacy risk of the model at different thresholds. We have seen that the model presents a considerable privacy risk, since the adversary can infer a considerable percentage of the training set members with a considerable precision if we choose the target model as a Random Forest classifier. We have also observed that the model choice has an impact on the privacy risk score, as the Logistic Regression classifier has a lower privacy risk score than the Random Forest classifier.\n",
    "\n",
    "This analysis can help to understand the privacy risks of a model and to take actions to mitigate these risks, such as implementing defenses against membership inference attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
